---
title: "Day Three"
author: "Jody Holland"
date: "2023-03-22"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = FALSE,
                      warning = FALSE,
                      message = FALSE)
```

<style>
body {
text-align: justify}
</style>

# R and Statistics Crash Course

*Day Three*

# GLM

![](https://imgs.xkcd.com/comics/linear_regression_2x.png)

The General Linear Model is a framework of models that function within the base concept of:

$$
Data = Model + Error
$$

As you will note this is similar to the Straight Line OLS model discussed earlier. In fact OLS is a type of GLM. But there are others, such as Logistic Regression, Poisson Regression etc. This others allow you to accomplish feats such as fitting a curve line with what appears to be exponential growth in the dependent or even work out probabilities with a binary categorical dependent variable (Probit, Logistic).

## When to move beyond OLS

Particularly for fitting a curve in place of a conventional OLS model, it may be that first you notice that it is necessary when you plot the residuals and the variation in the residuals changes over the course of change in the independent variable. You can check this by working out the Mean-Variance relationship at different points on the model.

## The building blocks of a GLM

GLMS have three building blocks:

### 1. Distribution:

The first building block of a GLM is the choice of probability distribution for the response variable. This distribution is chosen based on the nature of the response variable, and it determines the form of the likelihood function in the model. For example, if the response variable is continuous and normally distributed, we might choose a Gaussian distribution.

### 2. Family:

The second building block of a GLM is the family of probability distributions that is used to model the response variable. The family specifies the form of the likelihood function for the response variable, which is determined by the distribution chosen in the first building block. For example, if the response variable is binary, we might use the binomial family of distributions, which includes the logistic distribution. Other common families include the Poisson family (for count data) and the gamma family (for continuous data with positive values).

### 3. Link function:

The third building block of a GLM is the link function, which specifies the relationship between the linear predictor (the combination of predictor variables and regression coefficients) and the expected value of the response variable. The link function maps the range of the expected value of the response variable to the entire real line, allowing the linear predictor to take on any value. The choice of link function depends on the distribution and family chosen for the response variable. For example, if we are using the binomial family to model binary data, we might use the logistic link function.

# Example in R

```{r example-poisson}
# generate some random data
set.seed(123)
x = rnorm(100)
z = rnorm(100)
y = rpois(100, exp(x))

# fit a Poisson regression model with log link function
model = glm(y ~ x + z,
            family = poisson(link = "log"))

# print summary of model
summary(model)
```

We can also plot the results as follows

```{r poisson-plot}
plot(model)
```

# Practical

Using the data provided, fit a model and look for relationships

```{r libraries}
library(tidyverse)
library(readxl)
library(mvabund)
```

After loading our libaries its time to load and explore the data

```{r load-names}
# open data from xls to tibble
fauna_amphi = read_xls("fauna_amphi.xls") %>%
  as.tbl()

# view names of variables
fauna_amphi %>% names()
```

Lets determine the dependent variables and plot a box plot

```{r depednents}
fauna = mvabund(fauna_amphi[, 6:19])
boxplot(fauna)

```

We can now try and fit a model

```{r mean-variance}
model1=manyglm(fauna ~ fauna_amphi$Depth, family="poisson")
model2=manyglm(fauna ~ fauna_amphi$Season, family="poisson")
model3=manyglm(fauna ~ fauna_amphi$Depth*fauna_amphi$Season, family="poisson")
model4=manyglm(fauna ~ fauna_amphi$Mass + fauna_amphi$Depth*fauna_amphi$Season, family="poisson")

plot(model1)
plot(model2)
plot(model3)
plot(model4)

```
Uh oh, looks likt the residuals change other time, maybe poisson is the wrong family here

```{r negative-binomial}
model1b=manyglm(fauna ~ fauna_amphi$Depth, family="negative_binomial")
model2b=manyglm(fauna ~ fauna_amphi$Season, family="negative_binomial")
model3b=manyglm(fauna ~ fauna_amphi$Depth*fauna_amphi$Season, family="negative_binomial")
model4b=manyglm(fauna ~ fauna_amphi$Mass + fauna_amphi$Depth*fauna_amphi$Season, family="negative_binomial")

plot(model1b)
plot(model2b)
plot(model3b)
plot(model4b)

```

OK now thats more like it! Much better plot of the residual. Let's have a look at the summaries of these models

```{r summaries}
summary(model1b)
summary(model2b)
summary(model3b)
summary(model4b)
```
Now lets plot some stuff in ggplot2
```{r ggplot}

ggplot(fauna_amphi, aes(x=Depth,
                        y=Gammaropsis.ostroumowi)) +
  geom_boxplot()
ggplot(fauna_amphi, aes(x=Depth,
                        y=Amphilocus.manudens)) +
  geom_boxplot()

```

